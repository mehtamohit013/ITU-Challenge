{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# {'pkts_dropped': 0.0,\n",
    "#  'pkts_transmitted': 75.0,\n",
    "#  'timestamp': '1626810037874447104',\n",
    "#  'obj': 'simulation_pedestrian1',\n",
    "#  'pos_x': '11.417197',\n",
    "#  'pos_y': '37.027515',\n",
    "#  'pos_z': '7.4369965',\n",
    "#  'orien_x': '-0.0',\n",
    "#  'orien_y': '0.0',\n",
    "#  'orien_z': '0.9999752',\n",
    "#  'orien_w': '0.0070461035',\n",
    "#  'linear_acc_x': '',\n",
    "#  'linear_acc_y': '',\n",
    "#  'linear_acc_z': '',\n",
    "#  'linear_vel_x': '',\n",
    "#  'linear_vel_y': '',\n",
    "#  'linear_vel_z': '',\n",
    "#  'angular_acc_x': '',\n",
    "#  'angular_acc_y': '',\n",
    "#  'angular_acc_z': '',\n",
    "#  'angular_vel_x': '',\n",
    "#  'angular_vel_y': '',\n",
    "#  'angular_vel_z': '',\n",
    "#  'pkts_buffered': 0.0,\n",
    "#  'bit_rate': 4949598.859792932,\n",
    "#  'chosen_ue': 'simulation_pedestrian1',\n",
    "#  'packets': 14627.0,\n",
    "#  'channel_mag': array(0.00890296)}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.distributions import Categorical\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import copy\n",
    "import tqdm\n",
    "\n",
    "import gym\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device = torch.device(\"cpu\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gym Environment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import caviar_tools\n",
    "from beamselect_env import BeamSelectionEnv"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyper Params"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "reward_type = 'test'    # 'test' or 'train'\n",
    "epi = [0,10] #[start,end] \n",
    "epi_val = [490,499]\n",
    "\n",
    "\n",
    "gym_env = BeamSelectionEnv(epi, reward_type)\n",
    "gym_env_val = BeamSelectionEnv(epi_val)\n",
    "\n",
    "n_steps_per_epi = []\n",
    "\n",
    "for i in range(epi[1]+1):\n",
    "    n_steps_per_epi.append(caviar_tools.linecount([i, i]))\n",
    "\n",
    "for i in range(1, len(n_steps_per_epi)):\n",
    "    n_steps_per_epi[i] += n_steps_per_epi[i-1]\n",
    "\n",
    "n_steps = n_steps_per_epi[-1]\n",
    "n_steps_val = caviar_tools.linecount(epi_val)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(n_steps, n_steps_per_epi)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n_steps"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observation Space : X,Y,Z,pkts_dropped,pkts_transmitted,pkts_buffered,bit_rate\n",
    "\n",
    "Action Space : [3,64] -> [UE,Possible beams]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Replay Memory"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Transition = namedtuple('Transition',\n",
    "#                         ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "# class ReplayMemory(object):\n",
    "\n",
    "#     def __init__(self, capacity):\n",
    "#         self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "#     def push(self, *args):\n",
    "#         \"\"\"Save a transition\"\"\"\n",
    "#         self.memory.append(Transition(*args))\n",
    "\n",
    "#     def sample(self, batch_size):\n",
    "#         return random.sample(self.memory, batch_size)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.memory)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A2C"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class A2C(nn.Module):\n",
    "\n",
    "    def __init__(self, inputs:int=7, outputs:int=64*3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.affine =  nn.Sequential(\n",
    "            self.create_linear(inputs,16),\n",
    "            self.create_linear(16, 32),\n",
    "            self.create_linear(32,64),\n",
    "            self.create_linear(64,256)\n",
    "        )\n",
    "        \n",
    "        self.actor_linear = nn.Sequential(\n",
    "            self.create_linear(256,outputs)\n",
    "        )\n",
    "\n",
    "        self.critic_linear = nn.Sequential(\n",
    "            self.create_linear(256,1)\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    def create_linear(self,inp:int,out:int)-> nn.Module:\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(inp,out),\n",
    "            nn.ELU()\n",
    "            # nn.BatchNorm1d(out)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = self.affine(x)\n",
    "\n",
    "        # Actor - A2C implementation\n",
    "        x_action = self.actor_linear(x)\n",
    "        x_action = nn.Softmax(dim=-1)(x_action)\n",
    "\n",
    "        #  Critic\n",
    "        x_critic = self.critic_linear(x) \n",
    "\n",
    "        return x_action,x_critic\n",
    "\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Storage of actions and policy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Policy():\n",
    "    def __init__(self):\n",
    "         # Episode policy and reward history \n",
    "        self.rewards = []\n",
    "        self.saved_actions = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "policy = Policy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tmp = A2C()\n",
    "sample = torch.rand((1,7))\n",
    "out = tmp(sample)\n",
    "print(out[0].shape,out[1].shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparams"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "# EPS_START = 0.9\n",
    "# EPS_END = 0.3\n",
    "\n",
    "# # It depends on overall number of steps, basic intitution is that\n",
    "# # once steps_done == EPS_DECAY then the probablity of choosing \n",
    "# # random action is 33%; considering EPS_END is zero\n",
    "# # As for ep = [0,10]; approx ep is 80k therefore exploration can be reduced to 33% around 50k\n",
    "# # Also because of this factor smoothed accuracy matters more for training then seeing the average\n",
    "# EPS_DECAY = n_steps*0.3\n",
    "\n",
    "TARGET_UPDATE = 1000\n",
    "VAL_STEP = 60000"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Action"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "policy_net = A2C(11, 192).to(device)\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(),lr=3e-2)\n",
    "\n",
    "n_actions = 64*3\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state:torch.Tensor,val:bool=False):\n",
    "    if not val:\n",
    "        action_array, state_value = policy_net(state)\n",
    "        action_dist = Categorical(action_array)\n",
    "        action_value = action_dist.sample()\n",
    "\n",
    "        # Add log probability of our chosen action to our history    \n",
    "        # if policy.policy_history.dim() != 0:\n",
    "        policy.saved_actions.append(SavedAction(action_dist.log_prob(action_value).float(), state_value.float()))\n",
    "        # else:\n",
    "        #     policy.policy_history = (action_dist.log_prob(action_value))\n",
    "        return action_value\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            action = torch.max(policy_net(state)[0],dim=1)\n",
    "            return action.indices\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Tensorboard\n",
    "log_dir = './mini_logs/AC' \n",
    "\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimize Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def update_policy(n_step:int):\n",
    "\n",
    "    \"\"\"\n",
    "    Training code. Calculates actor and critic loss and performs backprop.\n",
    "    \"\"\"\n",
    "    R = 0\n",
    "    saved_actions = policy.saved_actions\n",
    "    policy_losses = [] # list to save actor (policy) loss\n",
    "    value_losses = [] # list to save critic (value) loss\n",
    "    returns = [] # list to save the true values\n",
    "\n",
    "    # calculate the true value using rewards returned from the environment\n",
    "    for r in policy.rewards[::-1]:\n",
    "        # calculate the discounted value\n",
    "        R = r + GAMMA * R\n",
    "        returns.insert(0, R)\n",
    "\n",
    "    returns = torch.tensor(returns).float()\n",
    "    \n",
    "    returns = (returns - returns.mean()) / (returns.std() + np.finfo(np.float32).eps)\n",
    "    for (log_prob, value), R in zip(saved_actions, returns):\n",
    "        advantage = R - value.item()\n",
    "        advantage = advantage.float()\n",
    "\n",
    "        # calculate actor (policy) loss \n",
    "        policy_losses.append(-log_prob * advantage)\n",
    "\n",
    "        # calculate critic (value) loss using L1 smooth loss\n",
    "        value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))\n",
    "\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # sum up all the values of policy_losses and value_losses\n",
    "    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "    loss = loss.float()\n",
    "\n",
    "    writer.add_scalar('Loss',loss.item(),n_step)\n",
    "    \n",
    "    # perform backprop\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # reset rewards and action buffer\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_actions[:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training loop"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def val(train_step:int,gym_env_val:gym.Env,n_steps_val:int=n_steps_val,\n",
    "        writer:SummaryWriter=writer):\n",
    "\n",
    "    state = torch.zeros((1,11), dtype=torch.float32)\n",
    "    running_reward = 0.0\n",
    "    for episode in tqdm.tqdm_notebook(range(n_steps_val),desc='Val'):\n",
    "        # Reset environment and record the starting state\n",
    "        done = False       \n",
    "\n",
    "        action = select_action(state, val = 'True')\n",
    "        # Step through environment using chosen action\n",
    "        state, reward, done, _ = gym_env_val.step([action.item()//64,action.item()%64])\n",
    "\n",
    "        state = state.astype(np.float32).reshape(1, state.shape[0])\n",
    "        state = torch.tensor(state)\n",
    "        running_reward +=reward\n",
    "        \n",
    "        # if done:\n",
    "        #     break\n",
    "    \n",
    "    writer.add_scalar('Val Reward',running_reward/n_steps_val,train_step)\n",
    "    gym_env_val.close()\n",
    "    print(f'Validation Reward {running_reward/n_steps_val}')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "state = torch.zeros((1,11), dtype=torch.float32)\n",
    "ovr_reward = 0.0\n",
    "episode_cnt = 0\n",
    "for episode in tqdm.tqdm_notebook(range(n_steps),desc='Train'):\n",
    "     # Reset environment and record the starting state\n",
    "    done = False       \n",
    "\n",
    "    action = select_action(state)\n",
    "\n",
    "    # Step through environment using chosen action\n",
    "    state, reward, done, _ = gym_env.step([action.data[0].item()//64,action.data[0].item()%64])\n",
    "\n",
    "    state = state.astype(np.float32).reshape(1, state.shape[0])\n",
    "    state = torch.tensor(state)\n",
    "\n",
    "    # Save reward\n",
    "    policy.rewards.append(reward)\n",
    "    writer.add_scalar('Rewards',reward.item(),episode)\n",
    "    ovr_reward+=reward.item()\n",
    "    \n",
    "\n",
    "    if episode == n_steps_per_epi[episode_cnt]:\n",
    "        update_policy(episode)\n",
    "        episode_cnt += 1\n",
    "\n",
    "    if (episode)%VAL_STEP == 0 or (episode == n_steps-1):\n",
    "        val(episode, copy.deepcopy(gym_env_val))\n",
    "\n",
    "\n",
    "writer.add_hparams(\n",
    "    {\n",
    "        'BATCH_SIZE' : BATCH_SIZE,\n",
    "        'GAMMA' : GAMMA,\n",
    "        'TARGET_UPDATE' :TARGET_UPDATE\n",
    "    },\n",
    "    {\n",
    "        'Overall Reward':ovr_reward,\n",
    "        'Average Reward': ovr_reward/n_steps\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f'Overall Train reward = {ovr_reward:.2f}. ' \\\n",
    "    f'Average Reward = {ovr_reward/n_steps:.4f}')\n",
    "gym_env.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('torch_rl': venv)"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "orig_nbformat": 2,
  "interpreter": {
   "hash": "f8337d322f02065436781250dc8881439cca8a58f1212dae4e4294dbbc4e4375"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}