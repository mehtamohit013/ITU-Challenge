{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import copy\n",
    "# import multiprocessing as mp\n",
    "from torch.multiprocessing import Pipe\n",
    "\n",
    "import gym\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import caviar_tools\n",
    "from beamselect_env import BeamSelectionEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohit/anaconda3/envs/ITU/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "reward_type = 'test'    # 'test' or 'train'\n",
    "epi = [0,0] #[start,end] \n",
    "epi_val = [500,500]\n",
    "\n",
    "# gym_env_train = BeamSelectionEnv(epi,reward_type)\n",
    "\n",
    "gym_env_ind = list()\n",
    "for i in range(epi[0],epi[1]+1):\n",
    "    gym_env_ind.append(BeamSelectionEnv([i,i],reward_type))\n",
    "\n",
    "gym_env_val = BeamSelectionEnv(epi_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_epi = list()\n",
    "n_steps_epi_val = list()\n",
    "for i in range(epi[0],epi[1]+1):\n",
    "    n_steps_epi.append(caviar_tools.linecount([i,i]))\n",
    "\n",
    "for i in range(epi_val[0],epi_val[1]+1):\n",
    "    n_steps_epi_val.append(caviar_tools.linecount([i,i]))\n",
    "\n",
    "n_steps = sum(n_steps_epi)\n",
    "n_steps_val = sum(n_steps_epi_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_method = 'ICM'\n",
    "env_id = None #BreakoutNoFrameskip-v4\n",
    "env_type = 'beamselect'\n",
    "env = gym_env_ind[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 0.95\n",
    "num_worker = 1\n",
    "\n",
    "num_step = int(128)\n",
    "\n",
    "ppo_eps = float(0.1)\n",
    "epoch = int(3)\n",
    "mini_batch = int(8)\n",
    "BATCH_SIZE = int(num_step * num_worker / mini_batch) #16\n",
    "learning_rate = float(1e-4)\n",
    "entropy_coef = float(0.001)\n",
    "gamma = float(0.99)\n",
    "eta = float(1)\n",
    "\n",
    "clip_grad_norm = float(0.5)\n",
    "\n",
    "pre_obs_norm_step = int(10)#int(10000)\n",
    "\n",
    "HISTORY_SIZE = 16\n",
    "STATES_USED = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = [HISTORY_SIZE,STATES_USED]  \n",
    "output_size = 192 #64*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_cur import *\n",
    "from agents_cur import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_rms = RunningMeanStd()\n",
    "obs_rms = RunningMeanStd(shape=(1, HISTORY_SIZE, 1, STATES_USED))\n",
    "\n",
    "\n",
    "discounted_reward = RewardForwardFilter(gamma)\n",
    "\n",
    "agent = ICMAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = agent(\n",
    "        input_size,\n",
    "        output_size,\n",
    "        num_worker,\n",
    "        num_step,\n",
    "        gamma,\n",
    "        lam=lam,\n",
    "        learning_rate=learning_rate,\n",
    "        ent_coef=entropy_coef,\n",
    "        clip_grad_norm=clip_grad_norm,\n",
    "        epoch=epoch,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        ppo_eps=ppo_eps,\n",
    "        eta=eta,\n",
    "        use_cuda=False,\n",
    "        use_gae=False,\n",
    "        use_noisy_net=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.zeros([1, HISTORY_SIZE, 1,STATES_USED])\n",
    "\n",
    "sample_episode = 0\n",
    "sample_rall = 0\n",
    "sample_step = 0\n",
    "sample_env_idx = 0\n",
    "sample_i_rall = 0\n",
    "global_update = 0\n",
    "global_step = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size:int=BATCH_SIZE):\n",
    "        return random.sample(self.memory, BATCH_SIZE)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = ReplayMemory(HISTORY_SIZE)\n",
    "\n",
    "for i in range(HISTORY_SIZE):\n",
    "    history.push(np.zeros((STATES_USED, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 1, 13)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = history.sample()\n",
    "np.array(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(action):\n",
    "    s, reward, done, info = env.step([action//64, action%64])\n",
    "    # print(type(s), s.shape)\n",
    "    history.push(s.astype(np.float))\n",
    "    \n",
    "    return [np.array(history.sample(BATCH_SIZE)), reward, done, done, reward]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to initailize observation normalization parameter.....\n",
      "End to initalize...\n"
     ]
    }
   ],
   "source": [
    "# normalize obs\n",
    "print('Start to initailize observation normalization parameter.....')\n",
    "next_obs = []\n",
    "steps = 0\n",
    "while steps < pre_obs_norm_step:\n",
    "    steps += num_worker\n",
    "    actions = np.random.randint(0, output_size, size=(num_worker,))\n",
    "\n",
    "    for action in actions:\n",
    "        s, r, d, rd, lr = run(action)\n",
    "        next_obs.append(s[:])\n",
    "        \n",
    "next_obs = np.stack(next_obs)\n",
    "obs_rms.update(next_obs)\n",
    "print('End to initalize...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9762128f99ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m                                     \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                                     \u001b[0mnum_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                                     num_worker)\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0madv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0madv\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mohit/ITU-Challenge/ITU-Challenge/utils_cur.py\u001b[0m in \u001b[0;36mmake_train_data\u001b[0;34m(reward, done, value, gamma, num_step, num_worker)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mrunning_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_step\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mrunning_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrunning_add\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    total_state, total_reward, total_done, total_next_state, total_action, total_int_reward, total_next_obs, total_values, total_policy = \\\n",
    "        [], [], [], [], [], [], [], [], []\n",
    "    global_step += (num_worker * num_step)\n",
    "    global_update += 1\n",
    "\n",
    "    # Step 1. n-step rollout\n",
    "    for _ in range(num_step):\n",
    "        actions, value, policy = agent.get_action((states - obs_rms.mean) / np.sqrt(obs_rms.var)) #Normalization\n",
    "\n",
    "        next_states, rewards, dones, real_dones, log_rewards, next_obs = [], [], [], [], [], []\n",
    "        for action in actions:\n",
    "            s, r, d, rd, lr = run(action)\n",
    "            next_states.append(s)\n",
    "            rewards.append(r)\n",
    "            dones.append(d)\n",
    "            real_dones.append(rd)\n",
    "            log_rewards.append(lr)\n",
    "\n",
    "        next_states = np.stack(next_states)\n",
    "        rewards = np.hstack(rewards)\n",
    "        dones = np.hstack(dones)\n",
    "        real_dones = np.hstack(real_dones)\n",
    "\n",
    "        # total reward = int reward\n",
    "        intrinsic_reward = agent.compute_intrinsic_reward(\n",
    "            (states - obs_rms.mean) / np.sqrt(obs_rms.var),\n",
    "            (next_states - obs_rms.mean) / np.sqrt(obs_rms.var),\n",
    "            actions)\n",
    "        sample_i_rall += intrinsic_reward[sample_env_idx]\n",
    "\n",
    "        total_int_reward.append(intrinsic_reward)\n",
    "        total_state.append(states)\n",
    "        total_next_state.append(next_states)\n",
    "        total_reward.append(rewards)\n",
    "        total_done.append(dones)\n",
    "        total_action.append(actions)\n",
    "        total_values.append(value)\n",
    "        total_policy.append(policy)\n",
    "\n",
    "        states = next_states[:, :, :, :]\n",
    "\n",
    "        sample_rall += log_rewards[sample_env_idx]\n",
    "\n",
    "        sample_step += 1\n",
    "        if real_dones[sample_env_idx]:\n",
    "            sample_episode += 1\n",
    "            # writer.add_scalar('data/reward_per_epi', sample_rall, sample_episode)\n",
    "            # writer.add_scalar('data/reward_per_rollout', sample_rall, global_update)\n",
    "            # writer.add_scalar('data/step', sample_step, sample_episode)\n",
    "            sample_rall = 0\n",
    "            sample_step = 0\n",
    "            sample_i_rall = 0\n",
    "\n",
    "    # calculate last next value\n",
    "    _, value, _ = agent.get_action((states - obs_rms.mean) / np.sqrt(obs_rms.var))\n",
    "    total_values.append(value)\n",
    "    # --------------------------------------------------\n",
    "\n",
    "    total_state = np.stack(total_state).transpose([1, 0, 2, 3, 4]).reshape([-1, HISTORY_SIZE, 1, STATES_USED])\n",
    "    total_next_state = np.stack(total_next_state).transpose([1, 0, 2, 3, 4]).reshape([-1, HISTORY_SIZE, 1, STATES_USED])\n",
    "    total_action = np.stack(total_action).transpose().reshape([-1])\n",
    "    total_done = np.stack(total_done).transpose()\n",
    "    total_values = np.stack(total_values).transpose()\n",
    "    total_logging_policy = torch.stack(total_policy).view(-1, output_size).cpu().numpy()\n",
    "\n",
    "    # Step 2. calculate intrinsic reward\n",
    "    # running mean intrinsic reward\n",
    "    total_int_reward = np.stack(total_int_reward).transpose()\n",
    "    total_reward_per_env = np.array([discounted_reward.update(reward_per_step) for reward_per_step in\n",
    "                                        total_int_reward.T])\n",
    "    mean, std, count = np.mean(total_reward_per_env), np.std(total_reward_per_env), len(total_reward_per_env)\n",
    "    reward_rms.update_from_moments(mean, std ** 2, count)\n",
    "\n",
    "    # normalize intrinsic reward\n",
    "    total_int_reward /= np.sqrt(reward_rms.var)\n",
    "    # writer.add_scalar('data/int_reward_per_epi', np.sum(total_int_reward) / num_worker, sample_episode)\n",
    "    # writer.add_scalar('data/int_reward_per_rollout', np.sum(total_int_reward) / num_worker, global_update)\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "\n",
    "    # logging Max action probability\n",
    "    # writer.add_scalar('data/max_prob', softmax(total_logging_policy).max(1).mean(), sample_episode)\n",
    "\n",
    "    # Step 3. make target and advantage\n",
    "    target, adv = make_train_data(total_int_reward,\n",
    "                                    np.zeros_like(total_int_reward),\n",
    "                                    total_values,\n",
    "                                    gamma,\n",
    "                                    num_step,\n",
    "                                    num_worker)\n",
    "\n",
    "    adv = (adv - np.mean(adv)) / (np.std(adv) + 1e-8)\n",
    "    # -----------------------------------------------\n",
    "\n",
    "    # Step 5. Training!\n",
    "    agent.train_model((total_state - obs_rms.mean) / np.sqrt(obs_rms.var),\n",
    "                        (total_next_state - obs_rms.mean) / np.sqrt(obs_rms.var),\n",
    "                        target, total_action,\n",
    "                        adv,\n",
    "                        total_policy)\n",
    "\n",
    "    if global_step % (num_worker * num_step * 100) == 0:\n",
    "        print('Now Global Step :{}'.format(global_step))\n",
    "        # torch.save(agent.model.state_dict(), model_path)\n",
    "        # torch.save(agent.icm.state_dict(), icm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4670286833580a43405122a6364ee48a0ddcd3c7511a5959f829e36e5f943148"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('ITU': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
