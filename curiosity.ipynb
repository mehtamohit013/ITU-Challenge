{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T09:22:28.858298Z",
     "start_time": "2021-10-15T09:22:27.034614Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import tqdm\n",
    "import pickle\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import copy\n",
    "# import multiprocessing as mp\n",
    "from torch.multiprocessing import Pipe\n",
    "import os\n",
    "\n",
    "import gym\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T09:22:28.865770Z",
     "start_time": "2021-10-15T09:22:28.861529Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T09:22:29.492361Z",
     "start_time": "2021-10-15T09:22:28.869038Z"
    }
   },
   "outputs": [],
   "source": [
    "import caviar_tools\n",
    "from beamselect_env import BeamSelectionEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T09:23:34.475641Z",
     "start_time": "2021-10-15T09:22:29.494689Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohit/anaconda3/envs/ITU/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "reward_type = 'test'    # 'test' or 'train'\n",
    "epi = [0,1] #[start,end] \n",
    "epi_val = [500,500]\n",
    "\n",
    "gym_env_train = BeamSelectionEnv(epi,reward_type)\n",
    "\n",
    "gym_env_ind = list()\n",
    "for i in range(epi[0],epi[1]+1):\n",
    "    gym_env_ind.append(BeamSelectionEnv([i,i],reward_type))\n",
    "    \n",
    "gym_env_ind_val = list()\n",
    "for i in range(epi_val[0],epi_val[1]+1):\n",
    "    gym_env_ind_val.append(BeamSelectionEnv([i,i]))\n",
    "\n",
    "# gym_env_val = BeamSelectionEnv(epi_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T09:23:35.729614Z",
     "start_time": "2021-10-15T09:23:34.477953Z"
    }
   },
   "outputs": [],
   "source": [
    "n_steps_epi = list()\n",
    "n_steps_epi_val = list()\n",
    "for i in range(epi[0],epi[1]+1):\n",
    "    n_steps_epi.append(caviar_tools.linecount([i,i]))\n",
    "\n",
    "for i in range(epi_val[0],epi_val[1]+1):\n",
    "    n_steps_epi_val.append(caviar_tools.linecount([i,i]))\n",
    "\n",
    "n_steps = sum(n_steps_epi)\n",
    "n_steps_val = sum(n_steps_epi_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T09:23:35.736981Z",
     "start_time": "2021-10-15T09:23:35.733189Z"
    }
   },
   "outputs": [],
   "source": [
    "train_method = 'ICM'\n",
    "env_id = None #BreakoutNoFrameskip-v4\n",
    "env_type = 'beamselect'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T09:23:35.847594Z",
     "start_time": "2021-10-15T09:23:35.742027Z"
    }
   },
   "outputs": [],
   "source": [
    "lam = 0.95\n",
    "num_worker = 1\n",
    "\n",
    "num_step = int(128)\n",
    "\n",
    "ppo_eps = float(0.1)\n",
    "epoch = int(3)\n",
    "mini_batch = int(8)\n",
    "BATCH_SIZE = int(num_step * num_worker / mini_batch) #16\n",
    "learning_rate = float(1e-4)\n",
    "entropy_coef = float(0.001)\n",
    "gamma = float(0.99)\n",
    "eta = float(1)\n",
    "\n",
    "clip_grad_norm = float(0.5)\n",
    "\n",
    "pre_obs_norm_step = int(1000)#int(10000)\n",
    "\n",
    "HISTORY_SIZE = 16\n",
    "STATES_USED = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T09:23:35.999507Z",
     "start_time": "2021-10-15T09:23:35.849735Z"
    }
   },
   "outputs": [],
   "source": [
    "model_path = './model_state_dict/model_curiosity'\n",
    "icm_path = './model_state_dict/icm_curiosity'\n",
    "os.makedirs('./model_state_dict',exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# States and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T09:23:36.097767Z",
     "start_time": "2021-10-15T09:23:36.002086Z"
    }
   },
   "outputs": [],
   "source": [
    "input_size = [HISTORY_SIZE,STATES_USED]  \n",
    "output_size = 192 #64*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T09:23:36.197808Z",
     "start_time": "2021-10-15T09:23:36.100324Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils_cur import *\n",
    "from agents_cur import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T09:23:36.295167Z",
     "start_time": "2021-10-15T09:23:36.200627Z"
    }
   },
   "outputs": [],
   "source": [
    "reward_rms = RunningMeanStd()\n",
    "obs_rms = RunningMeanStd(shape=(1, HISTORY_SIZE, 1, STATES_USED))\n",
    "\n",
    "\n",
    "discounted_reward = RewardForwardFilter(gamma)\n",
    "\n",
    "agent = ICMAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T09:23:36.415902Z",
     "start_time": "2021-10-15T09:23:36.297359Z"
    }
   },
   "outputs": [],
   "source": [
    "agent = agent(\n",
    "        input_size,\n",
    "        output_size,\n",
    "        num_worker,\n",
    "        num_step,\n",
    "        gamma,\n",
    "        lam=lam,\n",
    "        learning_rate=learning_rate,\n",
    "        ent_coef=entropy_coef,\n",
    "        clip_grad_norm=clip_grad_norm,\n",
    "        epoch=epoch,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        ppo_eps=ppo_eps,\n",
    "        eta=eta,\n",
    "        use_cuda=False,\n",
    "        use_gae=False,\n",
    "        use_noisy_net=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T09:23:36.516503Z",
     "start_time": "2021-10-15T09:23:36.418300Z"
    }
   },
   "outputs": [],
   "source": [
    "states = np.zeros([1, HISTORY_SIZE, 1,STATES_USED])\n",
    "\n",
    "sample_episode = 0\n",
    "sample_rall = 0\n",
    "sample_step = 0\n",
    "sample_env_idx = 0\n",
    "sample_i_rall = 0\n",
    "global_update = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T09:23:36.616046Z",
     "start_time": "2021-10-15T09:23:36.518892Z"
    }
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size:int=BATCH_SIZE):\n",
    "        return random.sample(self.memory, BATCH_SIZE)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T09:23:36.713730Z",
     "start_time": "2021-10-15T09:23:36.618171Z"
    }
   },
   "outputs": [],
   "source": [
    "history = ReplayMemory(HISTORY_SIZE)\n",
    "\n",
    "for i in range(HISTORY_SIZE):\n",
    "    history.push(np.zeros((STATES_USED, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T09:23:36.816739Z",
     "start_time": "2021-10-15T09:23:36.715966Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 1, 13)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = history.sample()\n",
    "np.array(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T09:23:36.970991Z",
     "start_time": "2021-10-15T09:23:36.819335Z"
    }
   },
   "outputs": [],
   "source": [
    "def run(action,env:gym.envs,history:ReplayMemory):\n",
    "    s, reward, done, info = env.step([action//64, action%64])\n",
    "    # print(type(s), s.shape)\n",
    "    history.push(s.astype(np.float))\n",
    "    \n",
    "    return [np.array(history.sample(BATCH_SIZE)), reward, done, done, reward]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "Mean and variance cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T09:23:38.895957Z",
     "start_time": "2021-10-15T09:23:36.977180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to initailize observation normalization parameter.....\n",
      "End to initalize...\n"
     ]
    }
   ],
   "source": [
    "# normalize obs\n",
    "print('Start to initailize observation normalization parameter.....')\n",
    "next_obs = []\n",
    "steps = 0\n",
    "while steps < pre_obs_norm_step:\n",
    "    steps += num_worker\n",
    "    actions = np.random.randint(0, output_size, size=(num_worker,))\n",
    "\n",
    "    for action in actions:\n",
    "        s, r, d, rd, lr = run(action,gym_env_train,history)\n",
    "        next_obs.append(s[:])\n",
    "        \n",
    "next_obs = np.stack(next_obs)\n",
    "obs_rms.update(next_obs)\n",
    "print('End to initalize...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T09:23:38.905136Z",
     "start_time": "2021-10-15T09:23:38.899577Z"
    }
   },
   "outputs": [],
   "source": [
    "f = open('obs_rms.pkl', 'wb')\n",
    "pickle.dump(obs_rms, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T09:23:39.023262Z",
     "start_time": "2021-10-15T09:23:38.909097Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "def val(gym_val:list):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        history = ReplayMemory(HISTORY_SIZE)\n",
    "        for i in range(HISTORY_SIZE):\n",
    "            history.push(np.zeros((STATES_USED, )))\n",
    "            \n",
    "        f = open('obs_rms.pkl', 'rb')\n",
    "        obs_rms = pickle.load(f)\n",
    "        \n",
    "        rall = 0\n",
    "        rd = False\n",
    "        intrinsic_reward_list = []\n",
    "        states = np.zeros([1, HISTORY_SIZE, 1,STATES_USED])\n",
    "        \n",
    "        ovr_val = 0\n",
    "        for episode in tqdm.notebook.tqdm(range(len(gym_val)),desc='Val'):\n",
    "            env = copy.deepcopy(gym_val[episode])\n",
    "            rall=0\n",
    "            for steps in range(n_steps_epi_val[episode]):\n",
    "                \n",
    "                actions, value, policy = agent.get_action(\n",
    "                    (states - obs_rms.mean) / np.sqrt(obs_rms.var)\n",
    "                )\n",
    "                \n",
    "                next_states, rewards, dones, real_dones, log_rewards, next_obs = [], [], [],\\\n",
    "                    [], [], []\n",
    "                \n",
    "                for action in actions:\n",
    "                    s, r, d, rd, lr = run(action,env,history)\n",
    "                    rall += r\n",
    "                    next_states.append(s)\n",
    "                \n",
    "                next_states = np.stack(next_states)\n",
    "                states = next_states[:, :, :, :]\n",
    "                \n",
    "            print(f'Running avg for episode {epi_val[0]+episode} is {rall/steps}')\n",
    "            ovr_val +=rall\n",
    "        \n",
    "        print(f'Overall mean reward is {ovr_val/n_steps_val} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T09:23:41.723581Z",
     "start_time": "2021-10-15T09:23:39.026102Z"
    }
   },
   "outputs": [],
   "source": [
    "log_dir = './curiosity_log/' \n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T09:26:29.279201Z",
     "start_time": "2021-10-15T09:23:41.725874Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b53901dbba94543973ecb5f55273c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running avg for episode 500 is -0.14304728138812808\n",
      "Overall mean reward is -0.14302775541469262 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38dd24012f7c4db4920b763f44acc7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epi:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e188db0d3b3843b28818e3416fd64af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward : -0.1599843024590628, Running: -0.1599843024590628\n",
      "Total reward : -0.17224778473733654, Running: -0.16611604359819968\n",
      "Total reward : -0.1733521785122858, Running: -0.1685280885695617\n",
      "Total reward : -0.175891234008729, Running: -0.17036887492935351\n",
      "Total reward : -0.17306979610755482, Running: -0.17090905916499377\n",
      "Total reward : -0.17491990089620266, Running: -0.1715775327868619\n",
      "Total reward : -0.17449225939979354, Running: -0.17199392230299498\n",
      "Total reward : -0.18256594612637667, Running: -0.1733154252809177\n",
      "Total reward : -0.17140950615813308, Running: -0.1731036564894972\n",
      "Total reward : -0.16947599961681303, Running: -0.17274089080222876\n",
      "Total reward : -0.16937535316843738, Running: -0.17243493283552047\n",
      "Total reward : -0.1701699868241891, Running: -0.1722461873345762\n",
      "Total reward : -0.16824357053083336, Running: -0.17193829373428826\n",
      "Total reward : -0.17184443552811512, Running: -0.17193158957670446\n",
      "Total reward : -0.16822006379437854, Running: -0.1716841545245494\n",
      "Total reward : -0.16527654688339133, Running: -0.17128367904697703\n",
      "Total reward : -0.17331257154315666, Running: -0.17140302566439936\n",
      "Total reward : -0.17068869846327775, Running: -0.17136334081989257\n",
      "Total reward : -0.17690344736897387, Running: -0.17165492537510738\n",
      "Total reward : -0.169949008727038, Running: -0.1715696295427039\n",
      "Total reward : -0.17522264738556598, Running: -0.1717435827733164\n",
      "Total reward : -0.17003594472582667, Running: -0.17166596286206687\n",
      "Total reward : -0.1713773536793154, Running: -0.17165341463672984\n",
      "Total reward : -0.17297032982941257, Running: -0.17170828610309163\n",
      "Total reward : -0.15126050113156028, Running: -0.17089037470423038\n",
      "Total reward : -0.14436377697813488, Running: -0.1698701209455344\n",
      "Total reward : -0.12902363128663258, Running: -0.1683572879952047\n",
      "Total reward : -0.12833433116938645, Running: -0.1669278966799969\n",
      "Total reward : -0.11250263374677819, Running: -0.16505116347540313\n",
      "Total reward : -0.11941964395590805, Running: -0.1635301128247533\n",
      "Total reward : -0.11742044132776584, Running: -0.16204270406678595\n",
      "Total reward : -0.08871726937369279, Running: -0.1597512842326268\n",
      "Total reward : -0.1192371312747981, Running: -0.1585235826278441\n",
      "Total reward : -0.09137115187432457, Running: -0.15654851113509352\n",
      "Total reward : -0.10805345835891521, Running: -0.1551629381986313\n",
      "Total reward : -0.09365141719255818, Running: -0.1534542848373515\n",
      "Total reward : -0.08514958952808462, Running: -0.1516082119911551\n",
      "Total reward : -0.05052274993776652, Running: -0.14894806825290802\n",
      "Total reward : -0.08437590457855827, Running: -0.1472923717484375\n",
      "Total reward : -0.03401500022361704, Running: -0.144460437460317\n",
      "Total reward : -0.008719276084555408, Running: -0.14114967742676185\n",
      "Total reward : -0.07829894664974812, Running: -0.13965323145588057\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c722b7c9bc434b5ca2d622cf7662c898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running avg for episode 500 is -0.14217394386020935\n",
      "Overall mean reward is -0.14215453709746567 \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1e8bc3222740e9aafb685d3fb543ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward : -0.16354274018840473, Running: -0.16354274018840473\n",
      "Total reward : -0.17428364944798666, Running: -0.1689131948181957\n",
      "Total reward : -0.1738716963901395, Running: -0.1705660286755103\n",
      "Total reward : -0.17593349892881838, Running: -0.17190789623883732\n",
      "Total reward : -0.1718019318832189, Running: -0.17188670336771364\n",
      "Total reward : -0.1739223181850662, Running: -0.1722259725039391\n",
      "Total reward : -0.1716797094026008, Running: -0.17214793491803362\n",
      "Total reward : -0.18281357664427905, Running: -0.1734811401338143\n",
      "Total reward : -0.17394515784703557, Running: -0.17353269765750554\n",
      "Total reward : -0.16510509312963367, Running: -0.17268993720471834\n",
      "Total reward : -0.16904253998480953, Running: -0.17235835563927207\n",
      "Total reward : -0.17396841397167964, Running: -0.17249252716697272\n",
      "Total reward : -0.17103736252401353, Running: -0.17238059142520662\n",
      "Total reward : -0.1769056492570963, Running: -0.17270380984177017\n",
      "Total reward : -0.17197190084375813, Running: -0.17265501590856935\n",
      "Total reward : -0.17096163524609886, Running: -0.17254917961716496\n",
      "Total reward : -0.17749987189408856, Running: -0.17284039680992516\n",
      "Total reward : -0.17370964668512542, Running: -0.17288868846965852\n",
      "Total reward : -0.175662961874527, Running: -0.17303470285938846\n",
      "Total reward : -0.17600993174579593, Running: -0.17318346430370882\n",
      "Total reward : -0.1704487981274541, Running: -0.17305324210483955\n",
      "Total reward : -0.17623124424924183, Running: -0.17319769674776692\n",
      "Total reward : -0.17518585586100544, Running: -0.1732841384483425\n",
      "Total reward : -0.16814405465644663, Running: -0.17306996829034685\n",
      "Total reward : -0.16183364777121995, Running: -0.1726205154695818\n",
      "Total reward : -0.16553740645443077, Running: -0.1723480881997683\n",
      "Total reward : -0.15479542500850924, Running: -0.171697989563055\n",
      "Total reward : -0.164159731273081, Running: -0.17142876605269877\n",
      "Total reward : -0.16563380447208545, Running: -0.1712289397912983\n",
      "Total reward : -0.17206440784041524, Running: -0.17125678872626887\n",
      "Total reward : -0.16352355889762352, Running: -0.17100732969953839\n",
      "Total reward : -0.16976413181703004, Running: -0.17096847976571\n",
      "Total reward : -0.17730401634756032, Running: -0.17116046572273577\n",
      "Total reward : -0.17613918484687274, Running: -0.17130689863815157\n",
      "Total reward : -0.17843739206412462, Running: -0.1715106270217508\n",
      "Total reward : -0.17700320224613442, Running: -0.17166319855576145\n",
      "Total reward : -0.1797590993398709, Running: -0.17188200668506173\n",
      "Total reward : -0.1800330726548767, Running: -0.1720965084211095\n",
      "Total reward : -0.18013104717907336, Running: -0.17230252223541626\n",
      "Total reward : -0.1795894097808715, Running: -0.17248469442405262\n",
      "Total reward : -0.17855811160922636, Running: -0.17263282655052029\n",
      "Total reward : -0.176326277892925, Running: -0.1727207658681966\n",
      "Total reward : -0.17990736251023048, Running: -0.1728878960226625\n",
      "Total reward : -0.17936860702556784, Running: -0.17303518490909217\n",
      "Total reward : -0.17625945803453025, Running: -0.1731068354229908\n",
      "Total reward : -0.178134033428838, Running: -0.1732161223361614\n",
      "Total reward : -0.17652804991930876, Running: -0.17328658888048365\n",
      "Total reward : -0.17944066612435408, Running: -0.1734147988230643\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702e75542843471d8c8f80fc19f2c0d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running avg for episode 500 is -0.14158256214970136\n",
      "Overall mean reward is -0.14156323611064187 \n"
     ]
    }
   ],
   "source": [
    "running_total_reward = 0\n",
    "\n",
    "val(gym_env_ind_val)\n",
    "for episode in tqdm.notebook.tqdm(range(len(gym_env_ind)),desc='Epi'):\n",
    "    running_total_reward = 0\n",
    "    for global_step in tqdm.notebook.tqdm(range(0, n_steps_epi[episode], num_step), desc = 'Train'):\n",
    "        total_state, total_reward, total_done, total_next_state, total_action, total_int_reward, total_next_obs, total_values, total_policy =         [], [], [], [], [], [], [], [], []\n",
    "        global_update += 1\n",
    "\n",
    "        # Step 1. n-step rollout\n",
    "        for _ in range(num_step):\n",
    "            actions, value, policy = agent.get_action((states - obs_rms.mean) / np.sqrt(obs_rms.var)) #Normalization\n",
    "\n",
    "            next_states, rewards, dones, real_dones, log_rewards, next_obs = [], [], [], [], [], []\n",
    "\n",
    "            for action in actions:\n",
    "                s, r, d, rd, lr = run(action,gym_env_ind[episode],history)\n",
    "                next_states.append(s)\n",
    "                rewards.append(r)\n",
    "                dones.append(d)\n",
    "                real_dones.append(rd)\n",
    "                log_rewards.append(lr)\n",
    "\n",
    "            next_states = np.stack(next_states)\n",
    "            rewards = np.hstack(rewards)\n",
    "            dones = np.hstack(dones)\n",
    "            real_dones = np.hstack(real_dones)\n",
    "\n",
    "            # total reward = int reward\n",
    "            intrinsic_reward = agent.compute_intrinsic_reward(\n",
    "                (states - obs_rms.mean) / np.sqrt(obs_rms.var),\n",
    "                (next_states - obs_rms.mean) / np.sqrt(obs_rms.var),\n",
    "                actions)\n",
    "            sample_i_rall += intrinsic_reward[sample_env_idx]\n",
    "\n",
    "            total_int_reward.append(intrinsic_reward)\n",
    "            total_state.append(states)\n",
    "            total_next_state.append(next_states)\n",
    "            total_reward.append(rewards)\n",
    "            total_done.append(dones)\n",
    "            total_action.append(actions)\n",
    "            total_values.append(value)\n",
    "            total_policy.append(policy)\n",
    "\n",
    "            states = next_states[:, :, :, :]\n",
    "\n",
    "            sample_rall += log_rewards[sample_env_idx]\n",
    "\n",
    "            sample_step += 1\n",
    "            if real_dones[sample_env_idx]:\n",
    "                sample_episode += 1\n",
    "#                 writer.add_scalar('data/reward_per_epi', sample_rall, sample_episode)\n",
    "#                 writer.add_scalar('data/reward_per_rollout', sample_rall, global_update)\n",
    "#                 writer.add_scalar('data/step', sample_step, sample_episode)\n",
    "                sample_rall = 0\n",
    "                sample_step = 0\n",
    "                sample_i_rall = 0\n",
    "\n",
    "        # calculate last next value\n",
    "        _, value, _ = agent.get_action((states - obs_rms.mean) / np.sqrt(obs_rms.var))\n",
    "        total_values.append(value)\n",
    "        # --------------------------------------------------\n",
    "\n",
    "        total_state = np.stack(total_state).transpose([1, 0, 2, 3, 4]).reshape([-1, HISTORY_SIZE, 1, STATES_USED])\n",
    "        total_next_state = np.stack(total_next_state).transpose([1, 0, 2, 3, 4]).reshape([-1, HISTORY_SIZE, 1, STATES_USED])\n",
    "        total_action = np.stack(total_action).transpose().reshape([-1])\n",
    "        total_done = np.stack(total_done).transpose()\n",
    "        total_values = np.stack(total_values).transpose()\n",
    "        total_logging_policy = torch.stack(total_policy).view(-1, output_size).cpu().numpy()\n",
    "\n",
    "        # Step 2. calculate intrinsic reward\n",
    "        # running mean intrinsic reward\n",
    "        total_int_reward = np.stack(total_int_reward).transpose()\n",
    "        total_reward_per_env = np.array([discounted_reward.update(reward_per_step) for reward_per_step in\n",
    "                                            total_int_reward.T])\n",
    "        mean, std, count = np.mean(total_reward_per_env), np.std(total_reward_per_env), len(total_reward_per_env)\n",
    "        reward_rms.update_from_moments(mean, std ** 2, count)\n",
    "\n",
    "        # normalize intrinsic reward\n",
    "        total_int_reward /= np.sqrt(reward_rms.var)\n",
    "        # writer.add_scalar('data/int_reward_per_epi', np.sum(total_int_reward) / num_worker, sample_episode)\n",
    "        # writer.add_scalar('data/int_reward_per_rollout', np.sum(total_int_reward) / num_worker, global_update)\n",
    "        # -------------------------------------------------------------------------------------------\n",
    "\n",
    "        # logging Max action probability\n",
    "        # writer.add_scalar('data/max_prob', softmax(total_logging_policy).max(1).mean(), sample_episode)\n",
    "\n",
    "        # Step 3. make target and advantage\n",
    "        target, adv = make_train_data(total_int_reward,\n",
    "                                        np.zeros_like(total_int_reward),\n",
    "                                        total_values,\n",
    "                                        gamma,\n",
    "                                        num_step,\n",
    "                                        num_worker)\n",
    "\n",
    "        adv = (adv - np.mean(adv)) / (np.std(adv) + 1e-8)\n",
    "        # -----------------------------------------------\n",
    "\n",
    "        # Step 5. Training!\n",
    "        agent.train_model((total_state - obs_rms.mean) / np.sqrt(obs_rms.var),\n",
    "                            (total_next_state - obs_rms.mean) / np.sqrt(obs_rms.var),\n",
    "                            target, total_action,\n",
    "                            adv,\n",
    "                            total_policy)\n",
    "\n",
    "        running_total_reward += np.sum(total_reward)\n",
    "\n",
    "\n",
    "        if (global_step) % (num_worker * num_step) == 0:\n",
    "#             print('Now Global Step :{}'.format((global_step)))\n",
    "            print(f'Total reward : {np.mean(total_reward)}, Running: {running_total_reward/(global_step+num_step)}')\n",
    "    \n",
    "    torch.save(agent.model.state_dict(), model_path)\n",
    "    torch.save(agent.icm.state_dict(), icm_path)\n",
    "\n",
    "    val(gym_env_ind_val)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4670286833580a43405122a6364ee48a0ddcd3c7511a5959f829e36e5f943148"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
