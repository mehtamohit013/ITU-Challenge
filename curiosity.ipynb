{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import tqdm\n",
    "import pickle\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import copy\n",
    "# import multiprocessing as mp\n",
    "from torch.multiprocessing import Pipe\n",
    "\n",
    "import gym\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import caviar_tools\n",
    "from beamselect_env import BeamSelectionEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sundesh/Documents/git/ITU-Challenge/torch_rl/lib/python3.6/site-packages/gym/logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "reward_type = 'test'    # 'test' or 'train'\n",
    "epi = [0,0] #[start,end] \n",
    "epi_val = [500,500]\n",
    "\n",
    "gym_env_train = BeamSelectionEnv(epi,reward_type)\n",
    "\n",
    "# gym_env_ind = list()\n",
    "# for i in range(epi[0],epi[1]+1):\n",
    "#     gym_env_ind.append(BeamSelectionEnv([i,i],reward_type))\n",
    "\n",
    "gym_env_val = BeamSelectionEnv(epi_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_epi = list()\n",
    "n_steps_epi_val = list()\n",
    "for i in range(epi[0],epi[1]+1):\n",
    "    n_steps_epi.append(caviar_tools.linecount([i,i]))\n",
    "\n",
    "for i in range(epi_val[0],epi_val[1]+1):\n",
    "    n_steps_epi_val.append(caviar_tools.linecount([i,i]))\n",
    "\n",
    "n_steps = sum(n_steps_epi)\n",
    "n_steps_val = sum(n_steps_epi_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7326"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_method = 'ICM'\n",
    "env_id = None #BreakoutNoFrameskip-v4\n",
    "env_type = 'beamselect'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 0.95\n",
    "num_worker = 1\n",
    "\n",
    "num_step = int(128)\n",
    "\n",
    "ppo_eps = float(0.1)\n",
    "epoch = int(3)\n",
    "mini_batch = int(8)\n",
    "BATCH_SIZE = int(num_step * num_worker / mini_batch) #16\n",
    "learning_rate = float(1e-4)\n",
    "entropy_coef = float(0.001)\n",
    "gamma = float(0.99)\n",
    "eta = float(1)\n",
    "\n",
    "clip_grad_norm = float(0.5)\n",
    "\n",
    "pre_obs_norm_step = int(10)#int(10000)\n",
    "\n",
    "HISTORY_SIZE = 16\n",
    "STATES_USED = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './model_curiosity'\n",
    "icm_path = './icm_curiosity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = [HISTORY_SIZE,STATES_USED]  \n",
    "output_size = 192 #64*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_cur import *\n",
    "from agents_cur import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_rms = RunningMeanStd()\n",
    "obs_rms = RunningMeanStd(shape=(1, HISTORY_SIZE, 1, STATES_USED))\n",
    "\n",
    "\n",
    "discounted_reward = RewardForwardFilter(gamma)\n",
    "\n",
    "agent = ICMAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = agent(\n",
    "        input_size,\n",
    "        output_size,\n",
    "        num_worker,\n",
    "        num_step,\n",
    "        gamma,\n",
    "        lam=lam,\n",
    "        learning_rate=learning_rate,\n",
    "        ent_coef=entropy_coef,\n",
    "        clip_grad_norm=clip_grad_norm,\n",
    "        epoch=epoch,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        ppo_eps=ppo_eps,\n",
    "        eta=eta,\n",
    "        use_cuda=False,\n",
    "        use_gae=False,\n",
    "        use_noisy_net=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.zeros([1, HISTORY_SIZE, 1,STATES_USED])\n",
    "\n",
    "sample_episode = 0\n",
    "sample_rall = 0\n",
    "sample_step = 0\n",
    "sample_env_idx = 0\n",
    "sample_i_rall = 0\n",
    "global_update = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size:int=BATCH_SIZE):\n",
    "        return random.sample(self.memory, BATCH_SIZE)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = ReplayMemory(HISTORY_SIZE)\n",
    "\n",
    "for i in range(HISTORY_SIZE):\n",
    "    history.push(np.zeros((STATES_USED, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 1, 13)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = history.sample()\n",
    "np.array(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(action):\n",
    "    s, reward, done, info = gym_env_train.step([action//64, action%64])\n",
    "    # print(type(s), s.shape)\n",
    "    history.push(s.astype(np.float))\n",
    "    \n",
    "    return [np.array(history.sample(BATCH_SIZE)), reward, done, done, reward]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to initailize observation normalization parameter.....\n",
      "End to initalize...\n"
     ]
    }
   ],
   "source": [
    "# normalize obs\n",
    "print('Start to initailize observation normalization parameter.....')\n",
    "next_obs = []\n",
    "steps = 0\n",
    "while steps < pre_obs_norm_step:\n",
    "    steps += num_worker\n",
    "    actions = np.random.randint(0, output_size, size=(num_worker,))\n",
    "\n",
    "    for action in actions:\n",
    "        s, r, d, rd, lr = run(action)\n",
    "        next_obs.append(s[:])\n",
    "        \n",
    "next_obs = np.stack(next_obs)\n",
    "obs_rms.update(next_obs)\n",
    "print('End to initalize...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('obs_rms.pkl', 'wb')\n",
    "pickle.dump(obs_rms, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(env):\n",
    "    with torch.no_grad():\n",
    "        history = ReplayMemory(HISTORY_SIZE)\n",
    "        for i in range(HISTORY_SIZE):\n",
    "            history.push(np.zeros((STATES_USED, )))\n",
    "        f = open('obs_rms.pkl', 'rb')\n",
    "        obs_rms = pickle.load(f)\n",
    "        rall = 0\n",
    "        rd = False\n",
    "        intrinsic_reward_list = []\n",
    "        states = np.zeros([1, HISTORY_SIZE, 1,STATES_USED])\n",
    "        def run(action):\n",
    "            s, reward, done, info = env.step([action//64, action%64])\n",
    "            # print(type(s), s.shape)\n",
    "            history.push(s.astype(np.float))\n",
    "\n",
    "            return [np.array(history.sample(BATCH_SIZE)), reward, done, done, reward]\n",
    "\n",
    "        for steps in tqdm.tqdm_notebook(range(n_steps_val), desc = 'Test'):\n",
    "            actions, value, policy = agent.get_action((states - obs_rms.mean) / np.sqrt(obs_rms.var))\n",
    "\n",
    "            next_states, rewards, dones, real_dones, log_rewards, next_obs = [], [], [], [], [], []\n",
    "\n",
    "            for action in actions:\n",
    "                s, r, d, rd, lr = run(action)\n",
    "                rall += r\n",
    "                next_states.append(s)\n",
    "        #         next_obs = s[3, :, :].reshape([1, 1, 1,STATES_USED])\n",
    "\n",
    "            # total reward = int reward + ext Reward\n",
    "        #     intrinsic_reward = agent.compute_intrinsic_reward(next_obs)\n",
    "        #     intrinsic_reward_list.append(intrinsic_reward)\n",
    "            next_states = np.stack(next_states)\n",
    "            states = next_states[:, :, :, :]\n",
    "\n",
    "        #     if rd:\n",
    "        #         intrinsic_reward_list = (intrinsic_reward_list - np.mean(intrinsic_reward_list)) / np.std(\n",
    "        #             intrinsic_reward_list)\n",
    "        #         with open('int_reward', 'wb') as f:\n",
    "        #             pickle.dump(intrinsic_reward_list, f)\n",
    "        #         steps = 0\n",
    "        #         rall = 0\n",
    "        print(f\"Total Val Reward: {rall}, Avg Val Reward: {rall/n_steps_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sundesh/Documents/git/ITU-Challenge/torch_rl/lib/python3.6/site-packages/ipykernel_launcher.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e62214c65b44cba00a54627ea6a6fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/781292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Global Step :0\n",
      "Total reward : -0.1745413074885006, Running: -0.1745413074885006\n",
      "Now Global Step :128\n",
      "Total reward : -0.1753301597239157, Running: -0.17493573360620815\n",
      "Now Global Step :256\n",
      "Total reward : -0.17332172558166506, Running: -0.17439773093136046\n",
      "Now Global Step :384\n",
      "Total reward : -0.16979742797998054, Running: -0.17324765519351548\n",
      "Now Global Step :512\n",
      "Total reward : -0.1756781322172023, Running: -0.17373375059825286\n",
      "Now Global Step :640\n",
      "Total reward : -0.17073812526689772, Running: -0.17323447970969366\n",
      "Now Global Step :768\n",
      "Total reward : -0.17471033591051066, Running: -0.1734453163098104\n",
      "Now Global Step :896\n",
      "Total reward : -0.17856297673200872, Running: -0.17408502386258518\n",
      "Now Global Step :1024\n",
      "Total reward : -0.16595505578325648, Running: -0.1731816940759931\n",
      "Now Global Step :1152\n",
      "Total reward : -0.17051574225004545, Running: -0.17291509889339834\n",
      "Now Global Step :1280\n",
      "Total reward : -0.16829876976712663, Running: -0.1724954326091918\n",
      "Now Global Step :1408\n",
      "Total reward : -0.17548351559155814, Running: -0.172744439524389\n",
      "Now Global Step :1536\n",
      "Total reward : -0.17088783974932809, Running: -0.1726016241570766\n",
      "Now Global Step :1664\n",
      "Total reward : -0.16887639568451107, Running: -0.1723355364090362\n",
      "Now Global Step :1792\n",
      "Total reward : -0.16577660371469882, Running: -0.1718982742294137\n",
      "Now Global Step :1920\n",
      "Total reward : -0.16505456487804607, Running: -0.17147054239495324\n",
      "Now Global Step :2048\n",
      "Total reward : -0.17365097686037867, Running: -0.17159880324586063\n",
      "Now Global Step :2176\n",
      "Total reward : -0.17436759655510786, Running: -0.17175262509637435\n",
      "Now Global Step :2304\n",
      "Total reward : -0.17478972222577427, Running: -0.17191247231371118\n",
      "Now Global Step :2432\n",
      "Total reward : -0.1773749883513153, Running: -0.1721855981155914\n",
      "Now Global Step :2560\n",
      "Total reward : -0.175902679912016, Running: -0.17236260201065923\n",
      "Now Global Step :2688\n",
      "Total reward : -0.1723055183970953, Running: -0.17236000730095177\n",
      "Now Global Step :2816\n",
      "Total reward : -0.1705292104598375, Running: -0.17228040743829462\n",
      "Now Global Step :2944\n",
      "Total reward : -0.1657847142154344, Running: -0.1720097535540088\n",
      "Now Global Step :3072\n",
      "Total reward : -0.15353059982480682, Running: -0.1712705874048407\n",
      "Now Global Step :3200\n",
      "Total reward : -0.13892055257738217, Running: -0.17002635529609228\n",
      "Now Global Step :3328\n",
      "Total reward : -0.13229744154855536, Running: -0.1686289881202576\n",
      "Now Global Step :3456\n",
      "Total reward : -0.1058837127367778, Running: -0.16638808542799044\n",
      "Now Global Step :3584\n",
      "Total reward : -0.12904918849467742, Running: -0.1651005372578762\n",
      "Now Global Step :3712\n",
      "Total reward : -0.11935381537738542, Running: -0.16357564652852652\n",
      "Now Global Step :3840\n",
      "Total reward : -0.09452828108576958, Running: -0.16134831215940532\n",
      "Now Global Step :3968\n",
      "Total reward : -0.11399337530676354, Running: -0.15986847038276025\n",
      "Now Global Step :4096\n",
      "Total reward : -0.10038714362174372, Running: -0.15806600593545672\n",
      "Now Global Step :4224\n",
      "Total reward : -0.10258578496465123, Running: -0.15643423473043302\n",
      "Now Global Step :4352\n",
      "Total reward : -0.11426997723600335, Running: -0.15522954165916358\n",
      "Now Global Step :4480\n",
      "Total reward : -0.08770379624612248, Running: -0.15335382650880136\n",
      "Now Global Step :4608\n",
      "Total reward : -0.08280051733574209, Running: -0.1514469803149349\n",
      "Now Global Step :4736\n",
      "Total reward : -0.09136330458934268, Running: -0.14986583095373507\n",
      "Now Global Step :4864\n",
      "Total reward : -0.09901934221962488, Running: -0.14856207483234765\n",
      "Now Global Step :4992\n",
      "Total reward : -0.04575931115773847, Running: -0.1459920057404824\n",
      "Now Global Step :5120\n",
      "Total reward : 0.009830259889279625, Running: -0.14219146267634186\n",
      "Now Global Step :5248\n",
      "Total reward : -0.01944866385416428, Running: -0.13926901508533765\n",
      "Now Global Step :5376\n",
      "Total reward : -0.07269636769832827, Running: -0.13772081398331418\n",
      "Now Global Step :5504\n",
      "Total reward : -0.06603812779365441, Running: -0.13609166202445827\n",
      "Now Global Step :5632\n",
      "Total reward : -0.07507242186207819, Running: -0.1347356789097387\n",
      "Now Global Step :5760\n",
      "Total reward : -0.04139590019400081, Running: -0.13270655328548353\n",
      "Now Global Step :5888\n",
      "Total reward : -0.08614365085057327, Running: -0.13171585323367693\n",
      "Now Global Step :6016\n",
      "Total reward : -0.1160764401968617, Running: -0.13139003212874328\n",
      "Now Global Step :6144\n",
      "Total reward : -0.11922319654010305, Running: -0.13114172936162818\n",
      "Now Global Step :6272\n",
      "Total reward : -0.1134720524441646, Running: -0.1307883358232789\n",
      "Now Global Step :6400\n",
      "Total reward : -0.12249270717950612, Running: -0.13062567643810688\n",
      "Now Global Step :6528\n",
      "Total reward : -0.14120386627889303, Running: -0.1308291031658143\n",
      "Now Global Step :6656\n",
      "Total reward : -0.14448756611145683, Running: -0.1310868100138453\n",
      "Now Global Step :6784\n",
      "Total reward : -0.14140753929543265, Running: -0.131277934630171\n",
      "Now Global Step :6912\n",
      "Total reward : -0.14579396409100914, Running: -0.13154186243854987\n",
      "Now Global Step :7040\n",
      "Total reward : -0.12830626213661267, Running: -0.13148408386172955\n",
      "Now Global Step :7168\n",
      "Total reward : -0.12859885102593852, Running: -0.1314334657418034\n",
      "Now Global Step :7296\n",
      "Total reward : -0.14607322515358984, Running: -0.1316858753868342\n",
      "Now Global Step :7424\n",
      "Total reward : -0.14207399527571213, Running: -0.13186194521545924\n",
      "Now Global Step :7552\n",
      "Total reward : -0.15145666605003708, Running: -0.13218852389603555\n",
      "Now Global Step :7680\n",
      "Total reward : -0.15169579425532811, Running: -0.132508315213401\n",
      "Now Global Step :7808\n",
      "Total reward : -0.15828046424114248, Running: -0.1329239950364291\n",
      "Now Global Step :7936\n",
      "Total reward : -0.15169161623513797, Running: -0.13322189378561494\n",
      "Now Global Step :8064\n",
      "Total reward : -0.17339708416792146, Running: -0.1338496311353385\n",
      "Now Global Step :8192\n",
      "Total reward : -0.16899588439241595, Running: -0.1343903427239089\n",
      "Now Global Step :8320\n",
      "Total reward : -0.17126242363744007, Running: -0.13494901061653816\n",
      "Now Global Step :8448\n",
      "Total reward : -0.16614632314789102, Running: -0.1354146421468569\n",
      "Now Global Step :8576\n",
      "Total reward : -0.16570081655140956, Running: -0.1358600270645709\n",
      "Now Global Step :8704\n",
      "Total reward : -0.16628330808008812, Running: -0.1363009441807378\n",
      "Now Global Step :8832\n",
      "Total reward : -0.1707496586986882, Running: -0.13679306867385135\n",
      "Now Global Step :8960\n",
      "Total reward : -0.17444357191559715, Running: -0.13732335745190413\n",
      "Now Global Step :9088\n",
      "Total reward : -0.1574002784159592, Running: -0.1376022035764049\n",
      "Now Global Step :9216\n",
      "Total reward : -0.1588102331716319, Running: -0.13789272452976417\n",
      "Now Global Step :9344\n",
      "Total reward : -0.16790621970152345, Running: -0.1382983123023555\n",
      "Now Global Step :9472\n",
      "Total reward : -0.16953011276424695, Running: -0.13871473630851408\n",
      "Now Global Step :9600\n",
      "Total reward : -0.16897960021234876, Running: -0.13911295820198558\n",
      "Now Global Step :9728\n",
      "Total reward : -0.17402108623646464, Running: -0.1395663105141217\n",
      "Now Global Step :9856\n",
      "Total reward : -0.17564961837768578, Running: -0.14002891702519302\n",
      "Now Global Step :9984\n",
      "Total reward : -0.17283912241793625, Running: -0.14044423608079737\n",
      "Now Global Step :10112\n",
      "Total reward : -0.1757221086772705, Running: -0.14088520948825328\n",
      "Now Global Step :10240\n",
      "Total reward : -0.17647884618348647, Running: -0.14132463710177467\n",
      "Now Global Step :10368\n",
      "Total reward : -0.1798209277969659, Running: -0.14179410406147214\n",
      "Now Global Step :10496\n",
      "Total reward : -0.17771210884938127, Running: -0.1422268511071096\n",
      "Now Global Step :10624\n",
      "Total reward : -0.17956232144051798, Running: -0.14267132099203111\n",
      "Now Global Step :10752\n",
      "Total reward : -0.17955542326537027, Running: -0.14310525160701157\n",
      "Now Global Step :10880\n",
      "Total reward : -0.18497666474326294, Running: -0.14359212850394473\n",
      "Now Global Step :11008\n",
      "Total reward : -0.17368036668727774, Running: -0.14393797032214395\n",
      "Now Global Step :11136\n",
      "Total reward : -0.1786386039001162, Running: -0.14433229570371184\n",
      "Now Global Step :11264\n",
      "Total reward : -0.17586738974428726, Running: -0.14468662260304413\n",
      "Now Global Step :11392\n",
      "Total reward : -0.1798565919436233, Running: -0.14507740004016167\n",
      "Now Global Step :11520\n",
      "Total reward : -0.18014022707701088, Running: -0.14546270583177542\n",
      "Now Global Step :11648\n",
      "Total reward : -0.1785525267891782, Running: -0.1458223777987037\n",
      "Now Global Step :11776\n",
      "Total reward : -0.17957147832111328, Running: -0.1461852713527081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Global Step :11904\n",
      "Total reward : -0.17611100825976514, Running: -0.14650363025597465\n",
      "Now Global Step :12032\n",
      "Total reward : -0.18078088736811315, Running: -0.146864443488734\n",
      "Now Global Step :12160\n",
      "Total reward : -0.18042837582708923, Running: -0.14721406778392523\n",
      "Now Global Step :12288\n",
      "Total reward : -0.17955547243544667, Running: -0.1475474843267244\n",
      "Now Global Step :12416\n",
      "Total reward : -0.1803969478442048, Running: -0.14788268293404563\n",
      "Now Global Step :12544\n",
      "Total reward : -0.1809418658876413, Running: -0.14821661407499104\n",
      "Now Global Step :12672\n",
      "Total reward : -0.17929288701808732, Running: -0.14852737680442202\n",
      "Now Global Step :12800\n",
      "Total reward : -0.18088600485534234, Running: -0.14884775926037172\n",
      "Now Global Step :12928\n",
      "Total reward : -0.18341616729048085, Running: -0.14918666522145121\n",
      "Now Global Step :13056\n",
      "Total reward : -0.17577637172697225, Running: -0.14944481771179607\n",
      "Now Global Step :13184\n",
      "Total reward : -0.17845265613917732, Running: -0.1497237392351363\n",
      "Now Global Step :13312\n",
      "Total reward : -0.17182117575645797, Running: -0.14993419101152983\n",
      "Now Global Step :13440\n",
      "Total reward : -0.17080204197552984, Running: -0.15013105753005812\n",
      "Now Global Step :13568\n",
      "Total reward : -0.1697989055498248, Running: -0.15031486919379425\n",
      "Now Global Step :13696\n",
      "Total reward : -0.17232972978159433, Running: -0.15051871049553314\n",
      "Now Global Step :13824\n",
      "Total reward : -0.1637292439924924, Running: -0.15063990805055114\n",
      "Now Global Step :13952\n",
      "Total reward : -0.16771181603799906, Running: -0.15079510721407338\n",
      "Now Global Step :14080\n",
      "Total reward : -0.17526063454333446, Running: -0.15101551737019286\n",
      "Now Global Step :14208\n",
      "Total reward : -0.1789145977578815, Running: -0.1512646163022258\n",
      "Now Global Step :14336\n",
      "Total reward : -0.17542694189224944, Running: -0.151478442192403\n",
      "Now Global Step :14464\n",
      "Total reward : -0.17606746832577913, Running: -0.1516941354040993\n",
      "Now Global Step :14592\n",
      "Total reward : -0.1745832162311911, Running: -0.15189317088955226\n",
      "Now Global Step :14720\n",
      "Total reward : -0.17550537874359942, Running: -0.15209672440553546\n",
      "Now Global Step :14848\n",
      "Total reward : -0.17481087699330294, Running: -0.15229086246184115\n",
      "Now Global Step :14976\n",
      "Total reward : -0.16275183253748476, Running: -0.15237951475061778\n",
      "Now Global Step :15104\n",
      "Total reward : -0.17479332490177246, Running: -0.1525678660964258\n",
      "Now Global Step :15232\n",
      "Total reward : -0.16497703216049664, Running: -0.1526712758136264\n",
      "Now Global Step :15360\n",
      "Total reward : -0.16341803209423247, Running: -0.15276009198123472\n",
      "Now Global Step :15488\n",
      "Total reward : -0.16006015118858716, Running: -0.15281992853211468\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Episodes over",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/git/ITU-Challenge/communications/ue.py\u001b[0m in \u001b[0;36mposition\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0mtmp_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/git/ITU-Challenge/communications/ue.py\u001b[0m in \u001b[0;36mposition\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episodeID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-1514a9c5e974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0ms_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym_env_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0;31m# print(type(s), s.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/ITU-Challenge/beamselect_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mbs_example_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs_example_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaviar_bs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_new_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mbs_example_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs_example_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaviar_bs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbs_example_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/ITU-Challenge/communications/base_station.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, target, action)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \"\"\"\n\u001b[1;32m    129\u001b[0m                 \u001b[0mpackets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traffic_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUEs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpacket_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUEs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mchannel_mag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH_mag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBit_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel_mag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m440.35\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mbuffered_packets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUEs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/ITU-Challenge/communications/base_station.py\u001b[0m in \u001b[0;36mH_mag\u001b[0;34m(self, target, index)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mMagnitude\u001b[0m \u001b[0mof\u001b[0m \u001b[0mchannel\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mchosen\u001b[0m \u001b[0mprecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \"\"\" \n\u001b[0;32m--> 113\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0mHt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchosen_precoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChannel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodebook_tx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m#noise = np.random.normal(0, .1, Ht.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/ITU-Challenge/communications/base_station.py\u001b[0m in \u001b[0;36mH_matrix\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mUpdate\u001b[0m \u001b[0mchannel\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mbased\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactual\u001b[0m \u001b[0mUE\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \"\"\"\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mposition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUEs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ULA'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChannel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mULAchannel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfrequency\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNTx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/ITU-Challenge/communications/ue.py\u001b[0m in \u001b[0;36mposition\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episodeID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Episodes over\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaviar_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositions_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episodeID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_all_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaviar_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episodeID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Episodes over"
     ]
    }
   ],
   "source": [
    "running_total_reward = 0\n",
    "for global_step in tqdm.tqdm_notebook(range(0, n_steps+100000000, num_step), desc = 'Train'):\n",
    "    total_state, total_reward, total_done, total_next_state, total_action, total_int_reward, total_next_obs, total_values, total_policy =         [], [], [], [], [], [], [], [], []\n",
    "    global_update += 1\n",
    "\n",
    "    # Step 1. n-step rollout\n",
    "    for _ in range(num_step):\n",
    "        actions, value, policy = agent.get_action((states - obs_rms.mean) / np.sqrt(obs_rms.var)) #Normalization\n",
    "\n",
    "        next_states, rewards, dones, real_dones, log_rewards, next_obs = [], [], [], [], [], []\n",
    "        \n",
    "        for action in actions:\n",
    "            s, r, d, rd, lr = run(action)\n",
    "            next_states.append(s)\n",
    "            rewards.append(r)\n",
    "            dones.append(d)\n",
    "            real_dones.append(rd)\n",
    "            log_rewards.append(lr)\n",
    "\n",
    "        next_states = np.stack(next_states)\n",
    "        rewards = np.hstack(rewards)\n",
    "        dones = np.hstack(dones)\n",
    "        real_dones = np.hstack(real_dones)\n",
    "\n",
    "        # total reward = int reward\n",
    "        intrinsic_reward = agent.compute_intrinsic_reward(\n",
    "            (states - obs_rms.mean) / np.sqrt(obs_rms.var),\n",
    "            (next_states - obs_rms.mean) / np.sqrt(obs_rms.var),\n",
    "            actions)\n",
    "        sample_i_rall += intrinsic_reward[sample_env_idx]\n",
    "\n",
    "        total_int_reward.append(intrinsic_reward)\n",
    "        total_state.append(states)\n",
    "        total_next_state.append(next_states)\n",
    "        total_reward.append(rewards)\n",
    "        total_done.append(dones)\n",
    "        total_action.append(actions)\n",
    "        total_values.append(value)\n",
    "        total_policy.append(policy)\n",
    "\n",
    "        states = next_states[:, :, :, :]\n",
    "\n",
    "        sample_rall += log_rewards[sample_env_idx]\n",
    "\n",
    "        sample_step += 1\n",
    "        if real_dones[sample_env_idx]:\n",
    "            sample_episode += 1\n",
    "            # writer.add_scalar('data/reward_per_epi', sample_rall, sample_episode)\n",
    "            # writer.add_scalar('data/reward_per_rollout', sample_rall, global_update)\n",
    "            # writer.add_scalar('data/step', sample_step, sample_episode)\n",
    "            sample_rall = 0\n",
    "            sample_step = 0\n",
    "            sample_i_rall = 0\n",
    "\n",
    "    # calculate last next value\n",
    "    _, value, _ = agent.get_action((states - obs_rms.mean) / np.sqrt(obs_rms.var))\n",
    "    total_values.append(value)\n",
    "    # --------------------------------------------------\n",
    "\n",
    "    total_state = np.stack(total_state).transpose([1, 0, 2, 3, 4]).reshape([-1, HISTORY_SIZE, 1, STATES_USED])\n",
    "    total_next_state = np.stack(total_next_state).transpose([1, 0, 2, 3, 4]).reshape([-1, HISTORY_SIZE, 1, STATES_USED])\n",
    "    total_action = np.stack(total_action).transpose().reshape([-1])\n",
    "    total_done = np.stack(total_done).transpose()\n",
    "    total_values = np.stack(total_values).transpose()\n",
    "    total_logging_policy = torch.stack(total_policy).view(-1, output_size).cpu().numpy()\n",
    "\n",
    "    # Step 2. calculate intrinsic reward\n",
    "    # running mean intrinsic reward\n",
    "    total_int_reward = np.stack(total_int_reward).transpose()\n",
    "    total_reward_per_env = np.array([discounted_reward.update(reward_per_step) for reward_per_step in\n",
    "                                        total_int_reward.T])\n",
    "    mean, std, count = np.mean(total_reward_per_env), np.std(total_reward_per_env), len(total_reward_per_env)\n",
    "    reward_rms.update_from_moments(mean, std ** 2, count)\n",
    "\n",
    "    # normalize intrinsic reward\n",
    "    total_int_reward /= np.sqrt(reward_rms.var)\n",
    "    # writer.add_scalar('data/int_reward_per_epi', np.sum(total_int_reward) / num_worker, sample_episode)\n",
    "    # writer.add_scalar('data/int_reward_per_rollout', np.sum(total_int_reward) / num_worker, global_update)\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "\n",
    "    # logging Max action probability\n",
    "    # writer.add_scalar('data/max_prob', softmax(total_logging_policy).max(1).mean(), sample_episode)\n",
    "\n",
    "    # Step 3. make target and advantage\n",
    "    target, adv = make_train_data(total_int_reward,\n",
    "                                    np.zeros_like(total_int_reward),\n",
    "                                    total_values,\n",
    "                                    gamma,\n",
    "                                    num_step,\n",
    "                                    num_worker)\n",
    "\n",
    "    adv = (adv - np.mean(adv)) / (np.std(adv) + 1e-8)\n",
    "    # -----------------------------------------------\n",
    "\n",
    "    # Step 5. Training!\n",
    "    agent.train_model((total_state - obs_rms.mean) / np.sqrt(obs_rms.var),\n",
    "                        (total_next_state - obs_rms.mean) / np.sqrt(obs_rms.var),\n",
    "                        target, total_action,\n",
    "                        adv,\n",
    "                        total_policy)\n",
    "\n",
    "    running_total_reward += np.sum(total_reward)\n",
    "\n",
    "    \n",
    "    if (global_step) % (num_worker * num_step) == 0:\n",
    "        print('Now Global Step :{}'.format((global_step)))\n",
    "        print(f'Total reward : {np.mean(total_reward)}, Running: {running_total_reward/(global_step+num_step)}')\n",
    "#         torch.save(agent.model.state_dict(), model_path)\n",
    "#         torch.save(agent.icm.state_dict(), icm_path)\n",
    "\n",
    "#     if (global_step % 1280) == 0:\n",
    "#         val(copy.deepcopy(gym_env_val))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4670286833580a43405122a6364ee48a0ddcd3c7511a5959f829e36e5f943148"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
