{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "475b73b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sundesh/Documents/git/ITU-Challenge/torch_rl/lib/python3.6/site-packages/gym/logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import copy\n",
    "# import multiprocessing as mp\n",
    "from torch.multiprocessing import Pipe\n",
    "\n",
    "import gym\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "import caviar_tools\n",
    "from beamselect_env import BeamSelectionEnv\n",
    "\n",
    "reward_type = 'test'    # 'test' or 'train'\n",
    "# epi = [0,9] #[start,end] \n",
    "epi_val = [500,500]\n",
    "\n",
    "# gym_env_train = BeamSelectionEnv(epi,reward_type)\n",
    "\n",
    "# gym_env_ind = list()\n",
    "# for i in range(epi[0],epi[1]+1):\n",
    "#     gym_env_ind.append(BeamSelectionEnv([i,i],reward_type))\n",
    "\n",
    "gym_env_val = BeamSelectionEnv(epi_val)\n",
    "\n",
    "# n_steps_epi = list()\n",
    "n_steps_epi_val = list()\n",
    "# for i in range(epi[0],epi[1]+1):\n",
    "#     n_steps_epi.append(caviar_tools.linecount([i,i]))\n",
    "\n",
    "for i in range(epi_val[0],epi_val[1]+1):\n",
    "    n_steps_epi_val.append(caviar_tools.linecount([i,i]))\n",
    "\n",
    "# n_steps = sum(n_steps_epi)\n",
    "n_steps_val = sum(n_steps_epi_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d479f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents_cur import *\n",
    "from utils_cur import *\n",
    "\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "837f46f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_method = 'ICM'\n",
    "env_id = None #BreakoutNoFrameskip-v4\n",
    "env_type = 'beamselect'\n",
    "env = gym_env_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ece318d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "HISTORY_SIZE = 16\n",
    "STATES_USED = 13\n",
    "\n",
    "input_size = [HISTORY_SIZE,STATES_USED]  \n",
    "output_size = 192 #64*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fae672ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './model_curiosity'\n",
    "icm_path = './icm_curiosity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d123a1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 0.95\n",
    "num_worker = 1\n",
    "\n",
    "num_step = int(128)\n",
    "\n",
    "ppo_eps = float(0.1)\n",
    "epoch = int(3)\n",
    "mini_batch = int(8)\n",
    "BATCH_SIZE = int(num_step * num_worker / mini_batch) #16\n",
    "learning_rate = float(1e-4)\n",
    "entropy_coef = float(0.001)\n",
    "gamma = float(0.99)\n",
    "eta = float(1)\n",
    "\n",
    "clip_grad_norm = float(0.5)\n",
    "\n",
    "pre_obs_norm_step = int(10)#int(10000)\n",
    "\n",
    "HISTORY_SIZE = 16\n",
    "STATES_USED = 13\n",
    "\n",
    "sticky_action = False\n",
    "life_done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15031d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_rms = RunningMeanStd(shape=(1, HISTORY_SIZE, 1, STATES_USED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95c86cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ICMAgent\n",
    "\n",
    "agent = agent(\n",
    "        input_size,\n",
    "        output_size,\n",
    "        num_worker,\n",
    "        num_step,\n",
    "        gamma,\n",
    "        lam=lam,\n",
    "        learning_rate=learning_rate,\n",
    "        ent_coef=entropy_coef,\n",
    "        clip_grad_norm=clip_grad_norm,\n",
    "        epoch=epoch,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        ppo_eps=ppo_eps,\n",
    "        eta=eta,\n",
    "        use_cuda=False,\n",
    "        use_gae=False,\n",
    "        use_noisy_net=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a16919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.zeros([1, HISTORY_SIZE, 1,STATES_USED])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4646470f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.model.load_state_dict(torch.load(model_path))\n",
    "agent.icm.load_state_dict(torch.load(icm_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f52da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size:int=BATCH_SIZE):\n",
    "        return random.sample(self.memory, BATCH_SIZE)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9668a0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = ReplayMemory(HISTORY_SIZE)\n",
    "\n",
    "for i in range(HISTORY_SIZE):\n",
    "    history.push(np.zeros((STATES_USED, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc0ba24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(action):\n",
    "    s, reward, done, info = env.step([action//64, action%64])\n",
    "    # print(type(s), s.shape)\n",
    "    history.push(s.astype(np.float))\n",
    "    \n",
    "    return [np.array(history.sample(BATCH_SIZE)), reward, done, done, reward]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd0c247a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to initailize observation normalization parameter.....\n",
      "End to initalize...\n"
     ]
    }
   ],
   "source": [
    "# normalize obs\n",
    "print('Start to initailize observation normalization parameter.....')\n",
    "next_obs = []\n",
    "steps = 0\n",
    "while steps < pre_obs_norm_step:\n",
    "    steps += num_worker\n",
    "    actions = np.random.randint(0, output_size, size=(num_worker,))\n",
    "\n",
    "    for action in actions:\n",
    "        s, r, d, rd, lr = run(action)\n",
    "        next_obs.append(s[:])\n",
    "        \n",
    "next_obs = np.stack(next_obs)\n",
    "obs_rms.update(next_obs)\n",
    "print('End to initalize...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "180f40f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: -1046.6951089905797, Avg Reward: -0.14287402525123938\n"
     ]
    }
   ],
   "source": [
    "steps = 0\n",
    "rall = 0\n",
    "rd = False\n",
    "intrinsic_reward_list = []\n",
    "while steps < n_steps_val:\n",
    "    steps += 1\n",
    "    actions, value, policy = agent.get_action((states - obs_rms.mean) / np.sqrt(obs_rms.var))\n",
    "\n",
    "    next_states, rewards, dones, real_dones, log_rewards, next_obs = [], [], [], [], [], []\n",
    "    for action in actions:\n",
    "        s, r, d, rd, lr = run(action)\n",
    "        rall += r\n",
    "        next_states.append(s)\n",
    "#         next_obs = s[3, :, :].reshape([1, 1, 1,STATES_USED])\n",
    "\n",
    "    # total reward = int reward + ext Reward\n",
    "#     intrinsic_reward = agent.compute_intrinsic_reward(next_obs)\n",
    "#     intrinsic_reward_list.append(intrinsic_reward)\n",
    "    next_states = np.stack(next_states)\n",
    "    states = next_states[:, :, :, :]\n",
    "\n",
    "#     if rd:\n",
    "#         intrinsic_reward_list = (intrinsic_reward_list - np.mean(intrinsic_reward_list)) / np.std(\n",
    "#             intrinsic_reward_list)\n",
    "#         with open('int_reward', 'wb') as f:\n",
    "#             pickle.dump(intrinsic_reward_list, f)\n",
    "#         steps = 0\n",
    "#         rall = 0\n",
    "\n",
    "print(f\"Total Reward: {rall}, Avg Reward: {rall/steps}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_rl",
   "language": "python",
   "name": "torch_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
